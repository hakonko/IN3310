{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input       : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden      : [[-0.4701929  0.5863904]]\n",
      "    Output (manual) : [[-0.3519801   0.52525216]]\n",
      "    RNN output      : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input       : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden      : [[-0.88883156  1.2364397 ]]\n",
      "    Output (manual) : [[-0.68424344  0.76074266]]\n",
      "    RNN output      : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input       : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden      : [[-1.3074702  1.8864892]]\n",
      "    Output (manual) : [[-0.8649416  0.9046636]]\n",
      "    RNN output      : [[-0.8649416  0.9046636]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "\n",
    "# output of the simple RNN\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "\n",
    "# manually computing the output\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('   Input       :', xt.numpy())\n",
    "\n",
    "    ht = torch.matmul(xt, torch.transpos(w_xh, 0, 1)) + b_xh\n",
    "    print('   Hidden      :', ht.detach().numpy())\n",
    "\n",
    "    if t > 0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "\n",
    "    print('    Output (manual) :', ot.detach().numpy())\n",
    "    print('    RNN output      :', output[:, t].detach().numpy)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 1 - Predicting Sentiment of IMDb movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Laster datasettet (tar første gang litt tid – det cacher lokalt)\n",
    "imdb = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = imdb['train']\n",
    "test_dataset = imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "torch.manual_seed(1)\n",
    "train_dataset, val_dataset = random_split(list(train_dataset), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 69006\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "for example in train_dataset:  # ikke unpack som tuple!\n",
    "    tokens = tokenizer(example[\"text\"])\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print(\"Vocab-size:\", len(token_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Behold de N vanligste ordene (valgfritt, f.eks. 10 000)\n",
    "max_vocab_size = 100000\n",
    "most_common = token_counts.most_common(max_vocab_size)\n",
    "\n",
    "# Spesielle tokens\n",
    "specials = [\"<PAD>\", \"<UNK>\"]\n",
    "word2index = {word: idx for idx, word in enumerate(specials)}\n",
    "index2word = {idx: word for idx, word in enumerate(specials)}\n",
    "\n",
    "# Start telling videre etter spesial-tokens\n",
    "for idx, (word, _) in enumerate(most_common, start=len(specials)):\n",
    "    word2index[word] = idx\n",
    "    index2word[idx] = word  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(text):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = [word2index.get(token, word2index[\"<UNK>\"]) for token in tokens]\n",
    "    return ids\n",
    "\n",
    "def label_pipeline(label):\n",
    "    return 1 if label == \"pos\" else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [35, 1739, 7, 449, 721, 6, 301, 4, 787, 9]\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "x = text_pipeline(sample[\"text\"])\n",
    "y = label_pipeline(sample[\"label\"])\n",
    "\n",
    "print(\"Input IDs:\", x[:10])\n",
    "print(\"Label:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first = True\n",
    "    )\n",
    "    return padded_text_list, label_list, lengths\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size = 4, shuffle=False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6357],\n",
      "        [6357],\n",
      "        [6357],\n",
      "        [6357]]) tensor([0, 0, 0, 0]) tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "print(text_batch, label_batch, length_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (in3310)",
   "language": "python",
   "name": "in3310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
