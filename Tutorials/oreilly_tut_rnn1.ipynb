{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 1 - Predicting Sentiment of IMDb movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Laster datasettet (tar første gang litt tid – det cacher lokalt)\n",
    "imdb = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = imdb['train']\n",
    "test_dataset = imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 69006\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "for example in train_dataset:  # ikke unpack som tuple!\n",
    "    tokens = tokenizer(example[\"text\"])\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print(\"Vocab-size:\", len(token_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Behold de N vanligste ordene (valgfritt, f.eks. 10 000)\n",
    "max_vocab_size = 100000\n",
    "most_common = token_counts.most_common(max_vocab_size)\n",
    "\n",
    "# Spesielle tokens\n",
    "specials = [\"<PAD>\", \"<UNK>\"]\n",
    "word2index = {word: idx for idx, word in enumerate(specials)}\n",
    "index2word = {idx: word for idx, word in enumerate(specials)}\n",
    "\n",
    "# Start telling videre etter spesial-tokens\n",
    "for idx, (word, _) in enumerate(most_common, start=len(specials)):\n",
    "    word2index[word] = idx\n",
    "    index2word[idx] = word  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return [word2index.get(token, word2index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "def label_pipeline(label):\n",
    "    return 1.0 if label == \"pos\" else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, label_list, lengths = [], [], []\n",
    "\n",
    "    for example in batch:\n",
    "        label = label_pipeline(example[\"label\"])\n",
    "        text_tensor = torch.tensor(text_pipeline(example[\"text\"]), dtype=torch.int64)\n",
    "        label_list.append(label)\n",
    "        text_list.append(text_tensor)\n",
    "        lengths.append(len(text_tensor))\n",
    "\n",
    "    padded_texts = nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=word2index[\"<PAD>\"])\n",
    "    labels = torch.tensor(label_list, dtype=torch.float32)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    return padded_texts, labels, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekst-batch shape: torch.Size([32, 975])\n",
      "Label-batch shape: torch.Size([32])\n",
      "Sekvenslengder: tensor([253, 330, 527, 111, 390, 188, 860, 271, 911, 190, 134, 111, 116, 102,\n",
      "        129,  43, 140, 246, 631,  98, 219, 403,  44, 166, 116, 130, 975, 400,\n",
      "        295,  65, 140, 178])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(train_dl))\n",
    "\n",
    "print(\"Tekst-batch shape:\", text_batch.shape)        # (batch_size, sekvenslengde)\n",
    "print(\"Label-batch shape:\", label_batch.shape)       # (batch_size,)\n",
    "print(\"Sekvenslengder:\", length_batch)               # (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (indekser): [3288, 304, 1659, 1072, 8568, 461, 45, 5, 163, 10824, 2289, 6418, 1344, 606, 38, 4, 107, 936, 8872, 9, 60, 656, 38, 11, 74, 16963, 11264, 3, 17880, 416, 40, 4, 368, 5, 50, 673, 3, 4, 543, 2763, 1130, 32, 207, 5, 2, 595, 5, 19020, 34, 15804, 14400, 19, 2, 18, 153, 21, 65, 81, 4, 50, 291, 5, 5016, 2, 899, 3, 2, 2745, 10, 51, 64, 1837, 2, 856, 3, 839, 49, 5, 2, 114, 15, 6, 137, 69, 89, 21, 1316, 388, 2, 107, 13, 3, 68, 1171, 10, 122, 8, 234, 30, 4, 116, 98, 74, 6, 959, 5, 4, 606, 976, 16, 2, 3254, 5, 2, 673, 3, 2, 1092, 19, 57, 11, 173, 162, 21, 726, 45, 4, 176, 16, 71, 2, 1147, 773, 7, 44, 35, 618, 243, 563, 6, 19020, 6, 3487, 2, 421, 44, 16905, 1845, 184, 69, 169, 45, 306, 12, 2, 595, 7, 166, 2, 1141, 14082, 40, 7, 4, 506, 3416, 847, 202, 2, 595, 13, 13038, 314, 258, 34, 5343, 18523, 4, 7758, 15, 35, 5935, 213, 22, 3, 4, 1103, 1370, 12, 2734, 49, 7059, 19, 165, 65, 6379, 1342, 150, 8832, 8809, 15, 2, 293, 670, 107, 3558, 7, 628, 19, 24, 12, 1082, 347, 2, 595, 2384, 49, 184, 84, 5, 41, 1400, 3, 93, 2, 18, 7904, 246, 15, 25, 126, 11, 7, 65, 4, 18524, 63, 17, 49, 673, 3, 1092, 19, 24, 12, 85, 5, 35, 584, 16, 4, 491, 65]\n",
      "Padding: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label: 0.0\n"
     ]
    }
   ],
   "source": [
    "seq_idx = 0  # f.eks. første i batch\n",
    "tokens = text_batch[seq_idx][:length_batch[seq_idx]]\n",
    "padding = text_batch[seq_idx][length_batch[seq_idx]:]\n",
    "\n",
    "print(\"Tokens (indekser):\", tokens.tolist())\n",
    "print(\"Padding:\", padding.tolist())\n",
    "print(\"Label:\", label_batch[seq_idx].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad-indeks: 0\n",
      "Unike verdier i hele batchen: tensor([    0,     2,     3,  ..., 62602, 62603, 62604])\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad-indeks:\", word2index[\"<PAD>\"])\n",
    "print(\"Unike verdier i hele batchen:\", torch.unique(text_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3430, -0.5329, -0.7423],\n",
      "         [-0.3842,  0.4307, -0.5028],\n",
      "         [ 0.5857, -0.2052,  2.7972],\n",
      "         [ 1.0885,  0.5652,  0.2847]],\n",
      "\n",
      "        [[ 0.5857, -0.2052,  2.7972],\n",
      "         [ 0.4700,  1.9600, -0.3665],\n",
      "         [-0.3842,  0.4307, -0.5028],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10, \n",
    "    embedding_dim=3,\n",
    "    padding_idx=0)\n",
    "\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3162],\n",
       "        [0.4356],\n",
       "        [0.4306],\n",
       "        [0.2232],\n",
       "        [0.2070]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = RNN(64, 32)\n",
    "print(model)\n",
    "\n",
    "model(torch.randn(5, 3, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (in3310)",
   "language": "python",
   "name": "in3310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
