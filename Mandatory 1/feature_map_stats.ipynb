{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ResNetData import ResNetDataPreprocessor, ResNetDataset\n",
    "from ResNetTrain import *\n",
    "from plotting import plot_losses\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**a)** Identify a few key layers in the pre-trained model that you want\n",
    "to analyze. Consider selecting layers from different stages of the network (early,\n",
    "middle, and late) to observe how the features evolve. As a starting point, you\n",
    "might choose a layer close to the input, one in the middle of the network, and\n",
    "one near the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "avgpool\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "BASE_PATH = Path.cwd()\n",
    "DATASET_PATH = BASE_PATH / 'mandatory1_data'\n",
    "\n",
    "model = resnet34(weights=ResNet34_Weights.DEFAULT).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Finding out which layers we have here\n",
    "for name, module in model.named_children():\n",
    "    print(name)\n",
    "\n",
    "# choosing layers to look at\n",
    "layer_names = ['layer1', 'layer3', 'layer4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Use PyTorch’s forward hooks to capture the output of the selected\n",
    "layers. A forward hook is a function that gets called every time a forward pass\n",
    "is made through the layer, allowing you to capture and store the output (i.e.,\n",
    "the feature map) of that layer. When you run a forward pass with an image, the\n",
    "hook will automatically capture feature maps from layers where it is registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.hooks.RemovableHandle at 0x7f2b104cc440>,\n",
       " <torch.utils.hooks.RemovableHandle at 0x7f2b103f7d10>,\n",
       " <torch.utils.hooks.RemovableHandle at 0x7f2b1035fda0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps = {}\n",
    "\n",
    "def hook_function(module, input, output, layer_name):\n",
    "    feature_maps[layer_name] = output.detach().cpu()\n",
    "\n",
    "hooks = []\n",
    "for name in layer_names:\n",
    "    layer = dict(model.named_children())[name]  # finding the right layer in the model\n",
    "    hook = layer.register_forward_hook(lambda module, input, output, name=name: hook_function(module, input, output, name))\n",
    "    hooks.append(hook)\n",
    "\n",
    "# looking at hooks\n",
    "hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After registering your hooks, run a few forward passes with different images\n",
    "from your dataset. This will trigger the hooks and store the feature maps for\n",
    "each registered layer. Ensure that your hooks save the captured feature maps in\n",
    "a way that you can easily access them later (e.g., storing them in a dictionary\n",
    "with layer names as keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Train, validation and test sets are disjoint\n",
      "Feature maps stored for image 1\n",
      "Feature maps stored for image 2\n",
      "Feature maps stored for image 3\n",
      "Feature maps stored for image 4\n",
      "Feature maps stored for image 5\n"
     ]
    }
   ],
   "source": [
    "feature_maps = {}\n",
    "\n",
    "# fetching data\n",
    "preprocessor = ResNetDataPreprocessor(dataset_path=DATASET_PATH, base_path=BASE_PATH)\n",
    "\n",
    "# defining our standard transform for a dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet needs 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# getting tha dataset from our existing code\n",
    "test_dataset = ResNetDataset(preprocessor.annotations_file, BASE_PATH, split=\"test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)  # using batch_size=1 for single images\n",
    "\n",
    "# getting feature maps from some images\n",
    "num_images = 5  # choosing the amount of images to analyse \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(test_loader):\n",
    "        image = image.to(device)  # if we have GPU, move the image there\n",
    "        model(image)  # a forward pass will activate the\n",
    "        \n",
    "        print(f\"Feature maps stored for image {i+1}\")\n",
    "        \n",
    "        if i+1 >= num_images:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Visualize the captured feature maps (from the previous task) to\n",
    "see the patterns learned by each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (layer_name, feature_map) in enumerate(feature_maps.items()):\n",
    "    feature_map = feature_map.squeeze(0)\n",
    "\n",
    "    num_channels = feature_map.shape[0]\n",
    "\n",
    "    num_to_show = min(4, num_channels)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(5, 2))\n",
    "    fig.suptitle(f\"{layer_name} Feature Maps for Image 5\")\n",
    "\n",
    "    for channel in range(num_to_show):\n",
    "        ax = axes[channel]\n",
    "        ax.imshow(feature_map[channel].cpu().numpy(), cmap=\"viridis\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Channel {channel}\")\n",
    "\n",
    "    for i in range(num_to_show, 4):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.savefig(f\"plots/feature_map_{layer_name}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Feature map Layer1](plots/feature_map_layer1.png)\n",
    "\n",
    "From this example, we notice that layer1 captures finer edges and textures. The activations here are more evenly distributed, so they respond to smaller variations such as noice and fine details.\n",
    "\n",
    "![Feature map Layer1](plots/feature_map_layer3.png)\n",
    "\n",
    "As we move deeper into the network (layer3, layer4) we notice that the feature maps become more sparse. The activations are now concentrated in specific regions, and they are most likely corresponding to high level structure or objects.\n",
    "\n",
    "![Feature map Layer1](plots/feature_map_layer4.png)\n",
    "\n",
    "Finally we are left with low-resolution maps suggesting that the network has abstracted itself away from the original details, with the \"heatmaps\" suggesting in which part of the image the network has captured most information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating folder for the feature maps\n",
    "save_dir = \"feature_maps\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# getting a batch of 10 images\n",
    "batch_size = 10\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# getting one batch of 10 images\n",
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the hooks\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_maps(feature_map, layer_name, image_idx, save_dir, num_filters=6):\n",
    "\n",
    "    feature_map = feature_map.squeeze(0)  # remove the batch dimension\n",
    "    num_channels = feature_map.shape[0]   # number of feature maps\n",
    "\n",
    "    # creating a folder for each image\n",
    "    image_dir = os.path.join(save_dir, f\"image_{image_idx}\")\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(min(num_filters, num_channels)):\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(feature_map[i].numpy(), cmap=\"viridis\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{layer_name} - Filter {i}\")\n",
    "\n",
    "        # saving PNG\n",
    "        save_path = os.path.join(image_dir, f\"{layer_name}_filter_{i}.png\")\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "        plt.close()  # closing up and freeing up memory\n",
    "\n",
    "# generating feature maps\n",
    "for img_idx in range(batch_size):\n",
    "    for layer_name, feature_map in feature_maps.items():\n",
    "        save_feature_maps(feature_map[img_idx].cpu(), layer_name, img_idx, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'layer1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# plotting\u001b[39;00m\n\u001b[1;32m     36\u001b[0m image_idx_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m]  \u001b[38;5;66;03m# plotting some images\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mplot_feature_maps_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_idx_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mplot_feature_maps_grid\u001b[0;34m(feature_maps, image_idx_list, layer_names, num_filters)\u001b[0m\n\u001b[1;32m     11\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(num_layers, num_images, figsize\u001b[38;5;241m=\u001b[39m(num_images \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, num_layers \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row, layer_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layer_names):\n\u001b[0;32m---> 14\u001b[0m     feature_map \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_maps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col, img_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_idx_list):\n\u001b[1;32m     17\u001b[0m         ax \u001b[38;5;241m=\u001b[39m axes[row, col] \u001b[38;5;28;01mif\u001b[39;00m num_layers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m axes[col]  \u001b[38;5;66;03m# handle one column\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'layer1'"
     ]
    }
   ],
   "source": [
    "def plot_feature_maps_grid(feature_maps, image_idx_list, layer_names, num_filters=5):\n",
    "    \"\"\"\n",
    "    Plot feature maps som et grid:\n",
    "    - Rader = forskjellige lag (f.eks. conv1, layer2, layer4)\n",
    "    - Kolonner = forskjellige bilder (f.eks. image_0, image_1, image_2)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_images = len(image_idx_list)\n",
    "    num_layers = len(layer_names)\n",
    "\n",
    "    fig, axes = plt.subplots(num_layers, num_images, figsize=(num_images * 3, num_layers * 3))\n",
    "\n",
    "    for row, layer_name in enumerate(layer_names):\n",
    "        feature_map = feature_maps[layer_name]\n",
    "\n",
    "        for col, img_idx in enumerate(image_idx_list):\n",
    "            ax = axes[row, col] if num_layers > 1 else axes[col]  # handle one column\n",
    "            fm = feature_map[img_idx]  # choose feature maps for this image\n",
    "\n",
    "            # choose filter\n",
    "            selected_filter = fm[0].cpu().numpy()  # first filter\n",
    "\n",
    "            ax.imshow(selected_filter, cmap=\"viridis\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            # Legg til labels for første rad og første kolonne\n",
    "            if row == 0:\n",
    "                ax.set_title(f\"Image {img_idx}\", fontsize=10)\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(layer_name, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plotting\n",
    "image_idx_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # plotting some images\n",
    "plot_feature_maps_grid(feature_maps, image_idx_list, layer_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modul layer1.1.relu: Gjennomsnittlig 30.86% ikke-positive verdier\n",
      "Modul layer2.0.relu: Gjennomsnittlig 47.71% ikke-positive verdier\n",
      "Modul layer3.4.relu: Gjennomsnittlig 65.19% ikke-positive verdier\n",
      "Modul layer4.0.relu: Gjennomsnittlig 69.22% ikke-positive verdier\n",
      "Modul layer4.1.relu: Gjennomsnittlig 66.77% ikke-positive verdier\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Hent alle moduler og velg 5 tilfeldige ReLU-moduler\n",
    "all_modules = list(model.named_modules())\n",
    "relu_modules = [name for name, _ in all_modules if 'relu' in name.lower()]\n",
    "selected_modules = sorted(random.sample(relu_modules, 5))\n",
    "\n",
    "# Dictionary for å lagre feature maps\n",
    "feature_maps = {}\n",
    "\n",
    "# Hook-funksjon for å lagre feature maps\n",
    "def hook_function(module, input, output, name):\n",
    "    if name in selected_modules:\n",
    "        if name not in feature_maps:\n",
    "            feature_maps[name] = []\n",
    "        feature_maps[name].append(output.detach().cpu())\n",
    "\n",
    "# Registrer hooks for utvalgte moduler\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if name in selected_modules:\n",
    "        # Viktig: Bruk default-argument for å fikse \"late binding\"\n",
    "        hook = module.register_forward_hook(\n",
    "            lambda mod, inp, out, name=name: hook_function(mod, inp, out, name)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "\n",
    "# Kjør modellen på 200 bilder\n",
    "batch_size = 200\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model(images)\n",
    "\n",
    "# Rapporter resultater\n",
    "for name in selected_modules:\n",
    "    if name in feature_maps:\n",
    "        non_positive_percentages = []\n",
    "        for feature_map in feature_maps[name]:\n",
    "            total_elements = feature_map.numel()\n",
    "            non_positive_count = torch.sum(feature_map <= 0).item()\n",
    "            percentage = (non_positive_count / total_elements) * 100\n",
    "            non_positive_percentages.append(percentage)\n",
    "        \n",
    "        avg_percentage = sum(non_positive_percentages) / len(non_positive_percentages)\n",
    "        print(f\"Modul {name}: Gjennomsnittlig {avg_percentage:.2f}% ikke-positive verdier\")\n",
    "        \n",
    "    # Slett feature maps for å spare lagringsplass\n",
    "    del feature_maps[name]\n",
    "\n",
    "# Fjern hooks når du er ferdig\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Train, validation and test sets are disjoint\n",
      "Module: layer1.0.relu, Average: 28.97%, Count: 2\n",
      "Module: layer2.0.relu, Average: 47.70%, Count: 2\n",
      "Module: layer3.0.relu, Average: 54.00%, Count: 2\n",
      "Module: layer4.0.relu, Average: 69.17%, Count: 2\n",
      "Module: relu, Average: 32.75%, Count: 1\n"
     ]
    }
   ],
   "source": [
    "# Hent alle moduler og velg 5 tilfeldige ReLU-moduler\n",
    "all_modules = list(model.named_modules())\n",
    "relu_modules = [name for name, _ in all_modules if 'relu' in name.lower()]\n",
    "#selected_modules = sorted(random.sample(relu_modules, 5))\n",
    "selected_modules = ['layer1.0.relu', 'layer2.0.relu', 'layer3.0.relu', 'layer4.0.relu', 'relu']\n",
    "\n",
    "# Dictionary for å lagre feature maps\n",
    "feature_maps = {}\n",
    "\n",
    "# dict to save statistics\n",
    "feature_stats = {name: {\"avg\": 0.0, \"count\": 0} for name in selected_modules}\n",
    "\n",
    "# new hook function with iterative updating function (method 2 in the problem text)\n",
    "def hook_function(module, input, output, name):\n",
    "\n",
    "    if name in selected_modules:\n",
    "        # calculating percent non-opositive values\n",
    "        feature_map = output.detach().cpu()\n",
    "        total_elements = feature_map.numel()\n",
    "        non_positive_count = torch.sum(feature_map <= 0).item()\n",
    "        percentage = (non_positive_count / total_elements) * 100\n",
    "        \n",
    "        # updating the average on the go\n",
    "        current_avg = feature_stats[name][\"avg\"]\n",
    "        current_count = feature_stats[name][\"count\"]\n",
    "        \n",
    "        # implementing the given formula: mt+1 = (mt * nt + ustep * nstep) / (nt + nstep)\n",
    "        new_count = current_count + 1\n",
    "        new_avg = (current_avg * current_count + percentage) / new_count\n",
    "        \n",
    "        # updating statistics\n",
    "        feature_stats[name][\"avg\"] = new_avg\n",
    "        feature_stats[name][\"count\"] = new_count\n",
    "\n",
    "\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if name in selected_modules:\n",
    "        hook = module.register_forward_hook(\n",
    "            lambda mod, inp, out, name=name: hook_function(mod, inp, out, name)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "\n",
    "\n",
    "# Kjør modellen på 200 bilder\n",
    "batch_size = 200\n",
    "\n",
    "preprocessor = ResNetDataPreprocessor(dataset_path=DATASET_PATH, base_path=BASE_PATH)\n",
    "train_dataset = ResNetDataset(preprocessor.annotations_file, BASE_PATH, split='train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "images, _ = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model(images)\n",
    "\n",
    "\n",
    "for name, item in feature_stats.items():\n",
    "    avg = item[\"avg\"]\n",
    "    count = item[\"count\"]\n",
    "    print(f'Module: {name}, Average: {avg:.2f}%, Count: {count}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (in3310)",
   "language": "python",
   "name": "in3310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
