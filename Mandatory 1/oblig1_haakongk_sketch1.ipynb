{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e559983-c9a7-43b1-9a60-6ed50e71fd3f",
   "metadata": {},
   "source": [
    "**Part a)** The first part is to split the data into three sets: training, validation, and test. Split each class separately so that there is an equal percentage of each class in all three sets. The number of total images in the validation is to be roughly 2000, in the test set 3000, and the remainder for the training set. It does not need to be exact. This is called a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b3ab57c-d1aa-4c7a-91bb-d6c1d77ac4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "from ResNet import ResNet\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380fc8e-f7d9-44b4-b69c-c5c1d1e79a59",
   "metadata": {},
   "source": [
    "First, we decide the classes. `.parts[-1]` gets out the last part in the directory, whilst `.iterdir()` iterates through directories, and `.is_dir()` sees to it, that it really is a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2bb678f4-9d26-43fc-9c32-4c2f8c0f4806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest', 'buildings', 'sea', 'glacier', 'mountain', 'street']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Path('/mnt/e/ml_projects/IN3310/2025/tut_data/mandatory1_data/')\n",
    "\n",
    "classes = [str(subdir.parts[-1]) for subdir in dataset.iterdir() if subdir.is_dir()]\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a0a9a-6901-443c-bd5f-a8c3d2d3d68b",
   "metadata": {},
   "source": [
    "Then, we create directories for `train, val, test` in the root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fd4883f-0afe-47fc-b2ab-881b8ce4193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('/mnt/e/ml_projects/IN3310/2025/tut_data/oblig1/')\n",
    "\n",
    "# the directories we want to create\n",
    "dirs = ['train', 'val', 'test']\n",
    "\n",
    "for dir_name in dirs:\n",
    "    # creating a path string\n",
    "    dir_path = base_path / dir_name\n",
    "    dir_path.mkdir(parents=True, exist_ok=True) # creating directory\n",
    "\n",
    "    # creating subdirectories of class names\n",
    "    for class_name in classes:\n",
    "        class_path = base_path / dir_name / class_name\n",
    "        class_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50850386-5c69-4b13-8f4d-a280eb732d11",
   "metadata": {},
   "source": [
    "Now we need to split the data into train, test, vals. A good way of doing this is to use `train_test_split` of the filenames and classes, and then fill our folders up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c35411d-187a-4f13-81f1-161bc1d65d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [] # container for image paths\n",
    "class_indices = [] # container for class indices\n",
    "\n",
    "for class_index, class_name in enumerate(classes):\n",
    "    class_path = dataset / class_name\n",
    "    for img_file in class_path.iterdir():\n",
    "        if img_file.is_file():\n",
    "            img_paths.append(img_file)\n",
    "            class_indices.append(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8433620-194f-47fa-b52f-e2579578ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, temp_imgs, train_indices, temp_indices = train_test_split(\n",
    "    img_paths, class_indices, test_size=0.3, stratify=class_indices, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c0c73ca-affb-408f-b841-63fe01e876c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs, test_imgs, val_indices, test_indices = train_test_split(\n",
    "    temp_imgs, temp_indices, test_size=0.6, stratify=temp_indices, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4abcc291-cf10-4e39-b974-c3bd7c647fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(img_paths, class_indices, split_name):\n",
    "    \n",
    "    for img_path, class_index in zip(img_paths, class_indices):\n",
    "        target_dir = base_path / split_name / classes[class_index]\n",
    "        target_file = target_dir / img_path.name\n",
    "\n",
    "        # copying the file if it's not already there\n",
    "        if not target_file.exists():\n",
    "            shutil.copy(img_path, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5305f96b-6939-4b56-b1ce-11c3c2af73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_images(train_imgs, train_indices, 'train')\n",
    "copy_images(val_imgs, val_indices, 'val')\n",
    "copy_images(test_imgs, test_indices, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f206b9-c2e8-4654-ab93-79dce611e88a",
   "metadata": {},
   "source": [
    "We now have folders with the data. Finally we can do a check for duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f151c51",
   "metadata": {},
   "source": [
    "**Part b)** Create a solution to verify that the dataset splits are disjoint. Ensure that no file appears in more than one of your training, validation, or\n",
    "test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25d7c9e8-611f-48b2-b086-49637bc7503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_no_duplicates(train_imgs, val_imgs, test_imgs):\n",
    "    train_set = set(train_imgs)\n",
    "    val_set = set(val_imgs)\n",
    "    test_set = set(test_imgs)\n",
    "\n",
    "    # using intersection to check for data overlaps\n",
    "    assert len(train_set.intersection(val_set)) == 0, 'Overlap between Train and Val'\n",
    "    assert len(train_set.intersection(test_set)) == 0, 'Overlap between Train and Test'\n",
    "    assert len(val_set.intersection(test_set)) == 0, 'Overlap between Val and Test'\n",
    "\n",
    "verify_no_duplicates(train_imgs, val_imgs, test_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8a0ee-1872-44ec-a549-1920ef618276",
   "metadata": {},
   "source": [
    "**Part c)** Develop and implement dataloaders for training, validation, and test sets. Please make one root path for the dataset, this makes it easier for us\n",
    "to check/debug your work. If there are multiple paths to the dataset that we need to change, it becomes tricky to change them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "346cb022-7f81-4364-bb36-a3147f831eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(), # converts to a torch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # normalizes to [-1, 1]\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # flipping image (P=50%)\n",
    "    transforms.RandomRotation(15),  # rotating ±15 degrees\n",
    "    transforms.RandomResizedCrop(150, scale=(0.8, 1.0)),  # cropping randomly and scaling\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Endrer farger\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b10f1b7-b298-4310-9a6e-cb2afafbed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=base_path / 'train', transform=transform)\n",
    "train_dataset_augm = datasets.ImageFolder(root=base_path / 'train', transform=augment_transform)\n",
    "val_dataset = datasets.ImageFolder(root=base_path / 'val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=base_path / 'test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "train_loader_augm = DataLoader(train_dataset_augm, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc20b69",
   "metadata": {},
   "source": [
    "2b) Use the dataloaders you created in Part 1 to feed\n",
    "the training data into the model of your choosing. Write code to perform the training process, ensuring that the model is optimized over the training data. Make sure to use the validation dataset to monitor performance during training. During training, monitor the model’s performance using accuracy on the validation set. This will give you an initial indication of how well your model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df918022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval() # put model in evaluation mode\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    ap_scores = []\n",
    "    for i in range(all_probs.shape[1]):\n",
    "        binary_labels = (all_labels == i).float()\n",
    "\n",
    "        ap = average_precision_score(\n",
    "            binary_labels.cpu().numpy(),\n",
    "            all_probs[:, i].cpu().numpy()\n",
    "        )\n",
    "        ap_scores.append(ap)\n",
    "\n",
    "    map_score = sum(ap_scores) / len(ap_scores)\n",
    "    \n",
    "    return accuracy, ap_scores, map_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e40eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, lr, num_epochs=10):\n",
    "    model = model.to(device)\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    map_scores = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # putting model into training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # zeroing out the optimizer\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        val_accuracy, _, map_score = evaluate_model(model, val_loader)\n",
    "\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "        map_scores.append(map_score)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, mAP: {map_score:.4f}\")\n",
    "\n",
    "    return train_accs, val_accs, map_scores\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab9481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Accuracy: 0.6216, Val Accuracy: 0.5093\n",
      "Epoch 2/20, Train Accuracy: 0.7569, Val Accuracy: 0.7882\n",
      "Epoch 3/20, Train Accuracy: 0.7922, Val Accuracy: 0.7055\n",
      "Epoch 4/20, Train Accuracy: 0.8094, Val Accuracy: 0.8322\n",
      "Epoch 5/20, Train Accuracy: 0.8365, Val Accuracy: 0.8209\n",
      "Epoch 6/20, Train Accuracy: 0.8435, Val Accuracy: 0.7104\n",
      "Epoch 7/20, Train Accuracy: 0.8551, Val Accuracy: 0.8112\n",
      "Epoch 8/20, Train Accuracy: 0.8640, Val Accuracy: 0.8420\n",
      "Epoch 9/20, Train Accuracy: 0.8709, Val Accuracy: 0.8190\n",
      "Epoch 10/20, Train Accuracy: 0.8800, Val Accuracy: 0.8415\n",
      "Epoch 11/20, Train Accuracy: 0.8912, Val Accuracy: 0.8390\n",
      "Epoch 12/20, Train Accuracy: 0.8989, Val Accuracy: 0.8513\n",
      "Epoch 13/20, Train Accuracy: 0.9114, Val Accuracy: 0.8591\n",
      "Epoch 14/20, Train Accuracy: 0.9207, Val Accuracy: 0.8249\n",
      "Epoch 15/20, Train Accuracy: 0.9288, Val Accuracy: 0.8601\n",
      "Epoch 16/20, Train Accuracy: 0.9389, Val Accuracy: 0.8567\n",
      "Epoch 17/20, Train Accuracy: 0.9481, Val Accuracy: 0.8547\n",
      "Epoch 18/20, Train Accuracy: 0.9535, Val Accuracy: 0.8523\n",
      "Epoch 19/20, Train Accuracy: 0.9589, Val Accuracy: 0.8513\n",
      "Epoch 20/20, Train Accuracy: 0.9644, Val Accuracy: 0.8635\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = ResNet(img_channels=3, num_layers=34, num_classes=len(classes))\n",
    "train_acc1, val_acc1, map_scores1 = train_model(model, train_loader, val_loader, criterion, optim.Adam, 0.001, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c241b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Accuracy: 0.5844, Val Accuracy: 0.6805\n",
      "Epoch 2/20, Train Accuracy: 0.7206, Val Accuracy: 0.5734\n",
      "Epoch 3/20, Train Accuracy: 0.7648, Val Accuracy: 0.7642\n",
      "Epoch 4/20, Train Accuracy: 0.7906, Val Accuracy: 0.8195\n",
      "Epoch 5/20, Train Accuracy: 0.8152, Val Accuracy: 0.8263\n",
      "Epoch 6/20, Train Accuracy: 0.8293, Val Accuracy: 0.7642\n",
      "Epoch 7/20, Train Accuracy: 0.8389, Val Accuracy: 0.7363\n",
      "Epoch 8/20, Train Accuracy: 0.8521, Val Accuracy: 0.8170\n",
      "Epoch 9/20, Train Accuracy: 0.8492, Val Accuracy: 0.8527\n",
      "Epoch 10/20, Train Accuracy: 0.8681, Val Accuracy: 0.8205\n",
      "Epoch 11/20, Train Accuracy: 0.8746, Val Accuracy: 0.8170\n",
      "Epoch 12/20, Train Accuracy: 0.8838, Val Accuracy: 0.8454\n",
      "Epoch 13/20, Train Accuracy: 0.8911, Val Accuracy: 0.8454\n",
      "Epoch 14/20, Train Accuracy: 0.8983, Val Accuracy: 0.8469\n",
      "Epoch 15/20, Train Accuracy: 0.9071, Val Accuracy: 0.8708\n",
      "Epoch 16/20, Train Accuracy: 0.9134, Val Accuracy: 0.8297\n",
      "Epoch 17/20, Train Accuracy: 0.9252, Val Accuracy: 0.8190\n",
      "Epoch 18/20, Train Accuracy: 0.9332, Val Accuracy: 0.8620\n",
      "Epoch 19/20, Train Accuracy: 0.9455, Val Accuracy: 0.8596\n",
      "Epoch 20/20, Train Accuracy: 0.9502, Val Accuracy: 0.8611\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(img_channels=3, num_layers=34, num_classes=len(classes))\n",
    "train_acc2, val_acc2, map_scores2 = train_model(model, train_loader, val_loader, criterion, optim.Adam, 0.001, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0079f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Accuracy: 0.5795, Val Accuracy: 0.6712\n",
      "Epoch 2/20, Train Accuracy: 0.7238, Val Accuracy: 0.7290\n",
      "Epoch 3/20, Train Accuracy: 0.7678, Val Accuracy: 0.7564\n",
      "Epoch 4/20, Train Accuracy: 0.7979, Val Accuracy: 0.8033\n",
      "Epoch 5/20, Train Accuracy: 0.8087, Val Accuracy: 0.7309\n",
      "Epoch 6/20, Train Accuracy: 0.8181, Val Accuracy: 0.8068\n",
      "Epoch 7/20, Train Accuracy: 0.8319, Val Accuracy: 0.7940\n",
      "Epoch 8/20, Train Accuracy: 0.8383, Val Accuracy: 0.8102\n",
      "Epoch 9/20, Train Accuracy: 0.8500, Val Accuracy: 0.8371\n",
      "Epoch 10/20, Train Accuracy: 0.8592, Val Accuracy: 0.8434\n",
      "Epoch 11/20, Train Accuracy: 0.8617, Val Accuracy: 0.8263\n",
      "Epoch 12/20, Train Accuracy: 0.8613, Val Accuracy: 0.8386\n",
      "Epoch 13/20, Train Accuracy: 0.8740, Val Accuracy: 0.7979\n",
      "Epoch 14/20, Train Accuracy: 0.8851, Val Accuracy: 0.8635\n",
      "Epoch 15/20, Train Accuracy: 0.8956, Val Accuracy: 0.8547\n",
      "Epoch 16/20, Train Accuracy: 0.9009, Val Accuracy: 0.8444\n",
      "Epoch 17/20, Train Accuracy: 0.9068, Val Accuracy: 0.8302\n",
      "Epoch 18/20, Train Accuracy: 0.9075, Val Accuracy: 0.8351\n",
      "Epoch 19/20, Train Accuracy: 0.9237, Val Accuracy: 0.8082\n",
      "Epoch 20/20, Train Accuracy: 0.9278, Val Accuracy: 0.8361\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(img_channels=3, num_layers=50, num_classes=len(classes))\n",
    "train_acc3, val_acc3, map_scores3 = train_model(model, train_loader, val_loader, criterion, optim.Adam, 0.001, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccdd9830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Accuracy: 0.5184, Val Accuracy: 0.6292\n",
      "Epoch 2/10, Train Accuracy: 0.6608, Val Accuracy: 0.7167\n",
      "Epoch 3/10, Train Accuracy: 0.7430, Val Accuracy: 0.7515\n",
      "Epoch 4/10, Train Accuracy: 0.7653, Val Accuracy: 0.7304\n",
      "Epoch 5/10, Train Accuracy: 0.7791, Val Accuracy: 0.7432\n",
      "Epoch 6/10, Train Accuracy: 0.8047, Val Accuracy: 0.7877\n",
      "Epoch 7/10, Train Accuracy: 0.8056, Val Accuracy: 0.7970\n",
      "Epoch 8/10, Train Accuracy: 0.8246, Val Accuracy: 0.8322\n",
      "Epoch 9/10, Train Accuracy: 0.8281, Val Accuracy: 0.8033\n",
      "Epoch 10/10, Train Accuracy: 0.8385, Val Accuracy: 0.7099\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(img_channels=3, num_layers=101, num_classes=len(classes))\n",
    "train_acc2, val_acc2 = train_model(model, train_loader, val_loader, criterion, optim.Adam, 0.001, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87fb0f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Accuracy: 0.4795, Val Accuracy: 0.5744\n",
      "Epoch 2/10, Train Accuracy: 0.6082, Val Accuracy: 0.6561\n",
      "Epoch 3/10, Train Accuracy: 0.6937, Val Accuracy: 0.7343\n",
      "Epoch 4/10, Train Accuracy: 0.7464, Val Accuracy: 0.7774\n",
      "Epoch 5/10, Train Accuracy: 0.7799, Val Accuracy: 0.8180\n",
      "Epoch 6/10, Train Accuracy: 0.7837, Val Accuracy: 0.7842\n",
      "Epoch 7/10, Train Accuracy: 0.8001, Val Accuracy: 0.7676\n",
      "Epoch 8/10, Train Accuracy: 0.7709, Val Accuracy: 0.7236\n",
      "Epoch 9/10, Train Accuracy: 0.8006, Val Accuracy: 0.7999\n",
      "Epoch 10/10, Train Accuracy: 0.8244, Val Accuracy: 0.7989\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(img_channels=3, num_layers=152, num_classes=len(classes))\n",
    "train_acc2, val_acc2 = train_model(model, train_loader, val_loader, criterion, optim.Adam, 0.001, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4e429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (in3310)",
   "language": "python",
   "name": "in3310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
